{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daafe427",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ist-nas/users/puntawatp/conda_envs/skelag/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 30, 1, 256, 1536)\n",
      "use_feats shape: torch.Size([1, 1, 256, 1536])\n"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "sys.path.insert(0, \"/home/mint/Dev/SkelAg/DiffSynth-Studio\")\n",
    "\n",
    "from diffsynth.models.wan_video_dit import DiTBlock, SelfAttention, rearrange, precompute_freqs_cis_3d, precompute_freqs_cis, sinusoidal_embedding_1d\n",
    "from diffsynth.models.wan_video_vae import Decoder3d, CausalConv3d, VideoVAE_, WanVideoVAE\n",
    "\n",
    "n_timesteps = 1000\n",
    "use_time = [1000]\n",
    "n_blocks = 30\n",
    "use_block = [30]\n",
    "B = 1\n",
    "D = 1536\n",
    "F = 256    # Flattened spatial-temporal dimension (1+T/4 * H/16 * W/16); H, W = 128\n",
    "feats_dim = (B, F, D)\n",
    "\n",
    "feats = np.zeros((n_timesteps, n_blocks, B, F, D), dtype=np.uint8)\n",
    "print(feats.shape)\n",
    "\n",
    "use_feats = []\n",
    "for t in use_time:\n",
    "    for b in use_block:\n",
    "        # print(f\"t: {t}, b: {b}\")\n",
    "        use_feats.append(feats[t-1, b-1])\n",
    "        # print(use_feats[0].shape)\n",
    "\n",
    "use_feats = th.tensor(np.stack(use_feats, axis=0))\n",
    "print(\"use_feats shape:\", use_feats.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57d99a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head_dim: 192\n",
      "x_flat shape: torch.Size([1, 256, 1536])\n",
      "SelfAttention: SelfAttention(\n",
      "  (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "  (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "  (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "  (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "  (norm_q): RMSNorm()\n",
      "  (norm_k): RMSNorm()\n",
      "  (attn): AttentionModule()\n",
      ")\n",
      "freqs.shape: torch.Size([256, 96])\n",
      "x_flat.shape: torch.Size([1, 256, 1536])\n",
      "x shape before rope: torch.Size([1, 256, 1536])\n",
      "x shape after rearrange: torch.Size([1, 256, 8, 192])\n",
      "x_out shape as complex: torch.Size([1, 256, 8, 96])\n",
      "freqs:  torch.Size([1, 256, 1, 96])\n",
      "x_out shape after rope: torch.Size([1, 256, 1536])\n",
      "x shape before rope: torch.Size([1, 256, 1536])\n",
      "x shape after rearrange: torch.Size([1, 256, 8, 192])\n",
      "x_out shape as complex: torch.Size([1, 256, 8, 96])\n",
      "freqs:  torch.Size([1, 256, 1, 96])\n",
      "x_out shape after rope: torch.Size([1, 256, 1536])\n",
      "out_flat shape: torch.Size([1, 256, 1536])\n",
      "out shape: torch.Size([1, 1, 256, 1536])\n",
      "out shape after head: torch.Size([1, 1, 256, 64])\n",
      "torch.Size([1, 1, 64, 256])\n",
      "out shape after joint_head: torch.Size([1, 1, 64, 1280])\n",
      "out_joint_combined shape: torch.Size([1, 1, 5, 256, 64])\n",
      "out shape after squeeze: torch.Size([1, 5, 256, 64])\n",
      "out_unpatch shape: torch.Size([1, 5, 16, 16, 8, 8])\n",
      "out to decoding:  torch.Size([1, 5, 16, 16, 8, 8])\n",
      "out to decoding:  torch.Size([1, 16, 16, 8, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VAE decoding:   0%|          | 0/1 [00:02<?, ?it/s]/s]\n",
      "Decoding joints:   0%|          | 0/5 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 84\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm.tqdm(\u001b[38;5;28mrange\u001b[39m(J), desc=\u001b[33m\"\u001b[39m\u001b[33mDecoding joints\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     video = \u001b[43mvideo_vae\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_unpatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout_unpatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtiled\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtiled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtile_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtile_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtile_stride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtile_stride\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m     out_joint.append(video)\n\u001b[32m     86\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mvideo shape:\u001b[39m\u001b[33m\"\u001b[39m, video.shape)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/SkelAg/DiffSynth-Studio/diffsynth/models/wan_video_vae.py:1241\u001b[39m, in \u001b[36mWanVideoVAE.decode\u001b[39m\u001b[34m(self, hidden_states, device, tiled, tile_size, tile_stride)\u001b[39m\n\u001b[32m   1239\u001b[39m hidden_state = hidden_state.unsqueeze(\u001b[32m0\u001b[39m)\n\u001b[32m   1240\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tiled:\n\u001b[32m-> \u001b[39m\u001b[32m1241\u001b[39m     video = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtiled_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtile_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtile_stride\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1242\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1243\u001b[39m     video = \u001b[38;5;28mself\u001b[39m.single_decode(hidden_state, device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/SkelAg/DiffSynth-Studio/diffsynth/models/wan_video_vae.py:1126\u001b[39m, in \u001b[36mWanVideoVAE.tiled_decode\u001b[39m\u001b[34m(self, hidden_states, device, tile_size, tile_stride)\u001b[39m\n\u001b[32m   1124\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m h, h_, w, w_ \u001b[38;5;129;01min\u001b[39;00m tqdm(tasks, desc=\u001b[33m\"\u001b[39m\u001b[33mVAE decoding\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1125\u001b[39m     hidden_states_batch = hidden_states[:, :, :, h:h_, w:w_].to(computation_device)\n\u001b[32m-> \u001b[39m\u001b[32m1126\u001b[39m     hidden_states_batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m)\u001b[49m.to(data_device)\n\u001b[32m   1128\u001b[39m     mask = \u001b[38;5;28mself\u001b[39m.build_mask(\n\u001b[32m   1129\u001b[39m         hidden_states_batch,\n\u001b[32m   1130\u001b[39m         is_bound=(h==\u001b[32m0\u001b[39m, h_>=H, w==\u001b[32m0\u001b[39m, w_>=W),\n\u001b[32m   1131\u001b[39m         border_width=((size_h - stride_h) * \u001b[38;5;28mself\u001b[39m.upsampling_factor, (size_w - stride_w) * \u001b[38;5;28mself\u001b[39m.upsampling_factor)\n\u001b[32m   1132\u001b[39m     ).to(dtype=hidden_states.dtype, device=data_device)\n\u001b[32m   1134\u001b[39m     target_h = h * \u001b[38;5;28mself\u001b[39m.upsampling_factor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/SkelAg/DiffSynth-Studio/diffsynth/models/wan_video_vae.py:1030\u001b[39m, in \u001b[36mVideoVAE_.decode\u001b[39m\u001b[34m(self, z, scale)\u001b[39m\n\u001b[32m   1026\u001b[39m         out = \u001b[38;5;28mself\u001b[39m.decoder(x[:, :, i:i + \u001b[32m1\u001b[39m, :, :],\n\u001b[32m   1027\u001b[39m                            feat_cache=\u001b[38;5;28mself\u001b[39m._feat_map,\n\u001b[32m   1028\u001b[39m                            feat_idx=\u001b[38;5;28mself\u001b[39m._conv_idx)\n\u001b[32m   1029\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1030\u001b[39m         out_ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1031\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mfeat_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_feat_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1032\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mfeat_idx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1033\u001b[39m         out = torch.cat([out, out_], \u001b[32m2\u001b[39m) \u001b[38;5;66;03m# may add tensor offload\u001b[39;00m\n\u001b[32m   1034\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ist-nas/users/puntawatp/conda_envs/skelag/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ist-nas/users/puntawatp/conda_envs/skelag/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/SkelAg/DiffSynth-Studio/diffsynth/models/wan_video_vae.py:817\u001b[39m, in \u001b[36mDecoder3d.forward\u001b[39m\u001b[34m(self, x, feat_cache, feat_idx)\u001b[39m\n\u001b[32m    815\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.upsamples:\n\u001b[32m    816\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m feat_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m817\u001b[39m         x = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeat_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeat_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    818\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    819\u001b[39m         x = layer(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ist-nas/users/puntawatp/conda_envs/skelag/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ist-nas/users/puntawatp/conda_envs/skelag/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/SkelAg/DiffSynth-Studio/diffsynth/models/wan_video_vae.py:159\u001b[39m, in \u001b[36mResample.forward\u001b[39m\u001b[34m(self, x, feat_cache, feat_idx)\u001b[39m\n\u001b[32m    157\u001b[39m t = x.shape[\u001b[32m2\u001b[39m]\n\u001b[32m    158\u001b[39m x = rearrange(x, \u001b[33m'\u001b[39m\u001b[33mb c t h w -> (b t) c h w\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    160\u001b[39m x = rearrange(x, \u001b[33m'\u001b[39m\u001b[33m(b t) c h w -> b c t h w\u001b[39m\u001b[33m'\u001b[39m, t=t)\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode == \u001b[33m'\u001b[39m\u001b[33mdownsample3d\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ist-nas/users/puntawatp/conda_envs/skelag/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ist-nas/users/puntawatp/conda_envs/skelag/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ist-nas/users/puntawatp/conda_envs/skelag/lib/python3.11/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ist-nas/users/puntawatp/conda_envs/skelag/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ist-nas/users/puntawatp/conda_envs/skelag/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ist-nas/users/puntawatp/conda_envs/skelag/lib/python3.11/site-packages/torch/nn/modules/conv.py:548\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ist-nas/users/puntawatp/conda_envs/skelag/lib/python3.11/site-packages/torch/nn/modules/conv.py:543\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    533\u001b[39m         F.pad(\n\u001b[32m    534\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    541\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    542\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from einops import repeat, reduce\n",
    "from PIL import Image\n",
    "def vae_output_to_video(vae_output, pattern=\"B C T H W\", min_value=-1, max_value=1):\n",
    "    # Transform a torch.Tensor to list of PIL.Image\n",
    "    if pattern != \"T H W C\":\n",
    "        vae_output = reduce(vae_output, f\"{pattern} -> T H W C\", reduction=\"mean\")\n",
    "    video = [vae_output_to_image(image, pattern=\"H W C\", min_value=min_value, max_value=max_value) for image in vae_output]\n",
    "    return video\n",
    "def vae_output_to_image(vae_output, pattern=\"B C H W\", min_value=-1, max_value=1):\n",
    "    # Transform a torch.Tensor to PIL.Image\n",
    "    if pattern != \"H W C\":\n",
    "        vae_output = reduce(vae_output, f\"{pattern} -> H W C\", reduction=\"mean\")\n",
    "    image = ((vae_output - min_value) * (255 / (max_value - min_value))).clip(0, 255)\n",
    "    image = image.to(device=\"cpu\", dtype=th.uint8)\n",
    "    image = Image.fromarray(image.numpy())\n",
    "    return image\n",
    "\n",
    "num_heads = 8\n",
    "D = 1536\n",
    "J = 5\n",
    "head_dim = D // num_heads  # 192\n",
    "print(\"head_dim:\", head_dim)\n",
    "\n",
    "x_flat = rearrange(use_feats, 'n b f d -> b (n f) d').cuda().float()   # [1, 4096, 1536]\n",
    "print(\"x_flat shape:\", x_flat.shape)\n",
    "\n",
    "self_attn = SelfAttention(dim=D, num_heads=num_heads).to(x_flat.device)\n",
    "# decoder = Decoder3d(dim=96, z_dim=16, dim_mult=[1, 2, 4, 4], num_res_blocks=2,\n",
    "#                             attn_scales=[], temporal_upsample=[False, True, True])\n",
    "decoder = Decoder3d().cuda()\n",
    "\n",
    "# 1D RoPE over sequence length\n",
    "freqs = precompute_freqs_cis(head_dim, end=x_flat.shape[1], theta=10000.0).to(x_flat.device)\n",
    "print(\"SelfAttention:\", self_attn)\n",
    "print(\"freqs.shape:\", freqs.shape)\n",
    "print(\"x_flat.shape:\", x_flat.shape)\n",
    "\n",
    "freqs = freqs[None, :, None, :]\n",
    "out_flat = self_attn(x_flat, freqs=freqs)       # [1, 4096, 1536]\n",
    "print(\"out_flat shape:\", out_flat.shape)\n",
    "out = rearrange(out_flat, 'b (n f) d -> n b f d', n=len(use_time) * len(use_block), f=F)\n",
    "print(\"out shape:\", out.shape)\n",
    "\n",
    "grid_size = [16, 4, 4]\n",
    "patch_size = [1, 2, 2]\n",
    "\n",
    "head = th.nn.Linear(D, 64).to(out.device)\n",
    "out = head(out)\n",
    "print(\"out shape after head:\", out.shape)\n",
    "\n",
    "joint_head = th.nn.Linear(F, F * J).to(out.device)\n",
    "out = rearrange(out, 'n b f d -> n b d f')\n",
    "print(out.shape)\n",
    "out = joint_head(out)\n",
    "print(\"out shape after joint_head:\", out.shape)\n",
    "out_joint_combined = rearrange(out, 'n b d (j f) -> n b j f d', j=J)\n",
    "print(\"out_joint_combined shape:\", out_joint_combined.shape)\n",
    "\n",
    "\n",
    "#TODO: Should be replace with some layers, Pooling, etc. to combined the multiple time/block features\n",
    "out = out_joint_combined.squeeze(0)  # [B, F, C]\n",
    "print(\"out shape after squeeze:\", out.shape)\n",
    "out_unpatch = rearrange(\n",
    "            out, 'b j (f h w) (x y z c) -> b j c (f x) (h y) (w z)',\n",
    "            f=grid_size[0], h=grid_size[1], w=grid_size[2], \n",
    "            x=patch_size[0], y=patch_size[1], z=patch_size[2], j = J\n",
    "        )\n",
    "print(\"out_unpatch shape:\", out_unpatch.shape)\n",
    "\n",
    "# tiled = False\n",
    "# tile_size = None\n",
    "# tile_stride = None\n",
    "tiled = True,\n",
    "tile_size = (30, 52)\n",
    "tile_stride = (15, 26)\n",
    "video_vae = WanVideoVAE().cpu()\n",
    "out_unpatch = out_unpatch.cpu()\n",
    "\n",
    "out_joint = []\n",
    "print(\"out to decoding: \", out_unpatch.shape)\n",
    "print(\"out to decoding: \", out_unpatch[0:1, 0, ...].shape)\n",
    "import tqdm\n",
    "for i in tqdm.tqdm(range(J), desc=\"Decoding joints\"):\n",
    "    video = video_vae.decode(out_unpatch[0:1, i, ...], device=out_unpatch.device, tiled=tiled, tile_size=tile_size, tile_stride=tile_stride)\n",
    "    out_joint.append(video)\n",
    "    print(\"video shape:\", video.shape)\n",
    "# video = vae_output_to_video(video)\n",
    "# print(\"video length:\", len(video))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39695516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(video.shape)\n",
    "plt.imshow(video[0][:, 0, ...].permute(1, 2, 0).cpu().detach().numpy())\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64704e8",
   "metadata": {},
   "source": [
    "Simple arch\n",
    "- Upsample + prediction head as 2N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce16ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[384, 384, 384, 192, 96]\n",
      "inp shape: torch.Size([1, 16, 16, 8, 8])\n",
      "conv1(inp).shape: torch.Size([1, 384, 16, 8, 8])\n",
      "Sequential(\n",
      "  (0): Resample(\n",
      "    (resample): Sequential(\n",
      "      (0): Upsample(scale_factor=(2.0, 2.0), mode='nearest-exact')\n",
      "      (1): Conv2d(384, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (time_conv): CausalConv3d(384, 768, kernel_size=(3, 1, 1), stride=(1, 1, 1))\n",
      "  )\n",
      "  (1): Resample(\n",
      "    (resample): Sequential(\n",
      "      (0): Upsample(scale_factor=(2.0, 2.0), mode='nearest-exact')\n",
      "      (1): Conv2d(384, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (time_conv): CausalConv3d(384, 768, kernel_size=(3, 1, 1), stride=(1, 1, 1))\n",
      "  )\n",
      "  (2): Resample(\n",
      "    (resample): Sequential(\n",
      "      (0): Upsample(scale_factor=(2.0, 2.0), mode='nearest-exact')\n",
      "      (1): Conv2d(192, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [192, 384, 3, 3], expected input[16, 192, 32, 32] to have 384 channels, but got 192 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mconv1(inp).shape:\u001b[39m\u001b[33m\"\u001b[39m, conv1(inp).shape)\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(upsamples)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[43mupsamples\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupsamples\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ist-nas/users/puntawatp/conda_envs/skelag/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ist-nas/users/puntawatp/conda_envs/skelag/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/SkelAg/DiffSynth-Studio/diffsynth/models/wan_video_vae.py:159\u001b[39m, in \u001b[36mResample.forward\u001b[39m\u001b[34m(self, x, feat_cache, feat_idx)\u001b[39m\n\u001b[32m    157\u001b[39m t = x.shape[\u001b[32m2\u001b[39m]\n\u001b[32m    158\u001b[39m x = rearrange(x, \u001b[33m'\u001b[39m\u001b[33mb c t h w -> (b t) c h w\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    160\u001b[39m x = rearrange(x, \u001b[33m'\u001b[39m\u001b[33m(b t) c h w -> b c t h w\u001b[39m\u001b[33m'\u001b[39m, t=t)\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode == \u001b[33m'\u001b[39m\u001b[33mdownsample3d\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ist-nas/users/puntawatp/conda_envs/skelag/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ist-nas/users/puntawatp/conda_envs/skelag/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ist-nas/users/puntawatp/conda_envs/skelag/lib/python3.11/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ist-nas/users/puntawatp/conda_envs/skelag/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ist-nas/users/puntawatp/conda_envs/skelag/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ist-nas/users/puntawatp/conda_envs/skelag/lib/python3.11/site-packages/torch/nn/modules/conv.py:548\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ist-nas/users/puntawatp/conda_envs/skelag/lib/python3.11/site-packages/torch/nn/modules/conv.py:543\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    533\u001b[39m         F.pad(\n\u001b[32m    534\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    541\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    542\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Given groups=1, weight of size [192, 384, 3, 3], expected input[16, 192, 32, 32] to have 384 channels, but got 192 channels instead"
     ]
    }
   ],
   "source": [
    "# # upsample block\n",
    "# if i != len(dim_mult) - 1:\n",
    "#     mode = 'upsample3d' if temperal_upsample[i] else 'upsample2d'\n",
    "#     upsamples.append(Resample(out_dim, mode=mode))\n",
    "#     scale *= 2.0\n",
    "temperal_upsample = [True, True, False]\n",
    "from diffsynth.models.wan_video_vae import Decoder3d, CausalConv3d, VideoVAE_, WanVideoVAE, Resample\n",
    "dim_mult = [1, 2, 4, 4]\n",
    "dim = 96\n",
    "z_dim = 16\n",
    "dims = [dim * u for u in [dim_mult[-1]] + dim_mult[::-1]]\n",
    "print(dims)\n",
    "cur_dim = dims[0]  # whatever your starting channels are\n",
    "\n",
    "upsamples = []\n",
    "for i, (in_dim, out_dim) in enumerate(zip(dims[:-1], dims[1:])):\n",
    "    if i != len(dim_mult) - 1:\n",
    "        mode = 'upsample3d' if temperal_upsample[i] else 'upsample2d'\n",
    "        upsamples.append(Resample(cur_dim, mode=mode))\n",
    "        cur_dim = cur_dim // 2   # <-- IMPORTANT: match Conv2d(cur_dim -> cur_dim//2)\n",
    "\n",
    "upsamples = th.nn.Sequential(*upsamples).cuda()\n",
    "conv1 = CausalConv3d(z_dim, dims[0], 3, padding=1).cuda()\n",
    "inp = th.randn(1, 16, 16, 8, 8).float().cuda()\n",
    "print(\"inp shape:\", inp.shape)\n",
    "print(\"conv1(inp).shape:\", conv1(inp).shape)\n",
    "print(upsamples)\n",
    "upsamples[1](upsamples[0](conv1(inp)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skelag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
