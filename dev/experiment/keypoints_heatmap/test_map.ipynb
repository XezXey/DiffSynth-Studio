{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daafe427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 30, 1, 256, 1536)\n",
      "use_feats shape: torch.Size([1, 1, 256, 1536])\n"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "sys.path.insert(0, \"/home/mint/Dev/SkelAg/DiffSynth-Studio\")\n",
    "\n",
    "from diffsynth.models.wan_video_dit import DiTBlock, SelfAttention, rearrange, precompute_freqs_cis_3d, precompute_freqs_cis, sinusoidal_embedding_1d\n",
    "from diffsynth.models.wan_video_vae import Decoder3d, CausalConv3d, VideoVAE_, WanVideoVAE\n",
    "\n",
    "n_timesteps = 1000\n",
    "use_time = [1000]\n",
    "n_blocks = 30\n",
    "use_block = [30]\n",
    "B = 1\n",
    "D = 1536\n",
    "F = 256    # Flattened spatial-temporal dimension (1+T/4 * H/16 * W/16); H, W = 128\n",
    "feats_dim = (B, F, D)\n",
    "\n",
    "feats = np.zeros((n_timesteps, n_blocks, B, F, D), dtype=np.uint8)\n",
    "print(feats.shape)\n",
    "\n",
    "use_feats = []\n",
    "for t in use_time:\n",
    "    for b in use_block:\n",
    "        # print(f\"t: {t}, b: {b}\")\n",
    "        use_feats.append(feats[t-1, b-1])\n",
    "        # print(use_feats[0].shape)\n",
    "\n",
    "use_feats = th.tensor(np.stack(use_feats, axis=0))\n",
    "print(\"use_feats shape:\", use_feats.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57d99a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head_dim: 192\n",
      "x_flat shape: torch.Size([1, 256, 1536])\n",
      "SelfAttention: SelfAttention(\n",
      "  (q): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "  (k): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "  (v): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "  (o): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "  (norm_q): RMSNorm()\n",
      "  (norm_k): RMSNorm()\n",
      "  (attn): AttentionModule()\n",
      ")\n",
      "freqs.shape: torch.Size([256, 96])\n",
      "x_flat.shape: torch.Size([1, 256, 1536])\n",
      "x shape before rope: torch.Size([1, 256, 1536])\n",
      "x shape after rearrange: torch.Size([1, 256, 8, 192])\n",
      "x_out shape as complex: torch.Size([1, 256, 8, 96])\n",
      "freqs:  torch.Size([1, 256, 1, 96])\n",
      "x_out shape after rope: torch.Size([1, 256, 1536])\n",
      "x shape before rope: torch.Size([1, 256, 1536])\n",
      "x shape after rearrange: torch.Size([1, 256, 8, 192])\n",
      "x_out shape as complex: torch.Size([1, 256, 8, 96])\n",
      "freqs:  torch.Size([1, 256, 1, 96])\n",
      "x_out shape after rope: torch.Size([1, 256, 1536])\n",
      "out_flat shape: torch.Size([1, 256, 1536])\n",
      "out shape: torch.Size([1, 1, 256, 1536])\n",
      "out shape after head: torch.Size([1, 1, 256, 64])\n",
      "torch.Size([1, 1, 64, 256])\n",
      "out shape after joint_head: torch.Size([1, 1, 64, 1280])\n",
      "out_joint_combined shape: torch.Size([1, 1, 5, 256, 64])\n",
      "out shape after squeeze: torch.Size([1, 5, 256, 64])\n",
      "out_unpatch shape: torch.Size([1, 5, 16, 16, 8, 8])\n",
      "out to decoding:  torch.Size([1, 5, 16, 16, 8, 8])\n",
      "out to decoding:  torch.Size([1, 16, 16, 8, 8])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 82\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mout to decoding: \u001b[39m\u001b[33m\"\u001b[39m, out_unpatch.shape)\n\u001b[32m     81\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mout to decoding: \u001b[39m\u001b[33m\"\u001b[39m, out_unpatch[\u001b[32m0\u001b[39m:\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m, ...].shape)\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm.tqdm(\u001b[38;5;28mrange\u001b[39m(J), desc=\u001b[33m\"\u001b[39m\u001b[33mDecoding joints\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[31mAssertionError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from einops import repeat, reduce\n",
    "from PIL import Image\n",
    "def vae_output_to_video(vae_output, pattern=\"B C T H W\", min_value=-1, max_value=1):\n",
    "    # Transform a torch.Tensor to list of PIL.Image\n",
    "    if pattern != \"T H W C\":\n",
    "        vae_output = reduce(vae_output, f\"{pattern} -> T H W C\", reduction=\"mean\")\n",
    "    video = [vae_output_to_image(image, pattern=\"H W C\", min_value=min_value, max_value=max_value) for image in vae_output]\n",
    "    return video\n",
    "def vae_output_to_image(vae_output, pattern=\"B C H W\", min_value=-1, max_value=1):\n",
    "    # Transform a torch.Tensor to PIL.Image\n",
    "    if pattern != \"H W C\":\n",
    "        vae_output = reduce(vae_output, f\"{pattern} -> H W C\", reduction=\"mean\")\n",
    "    image = ((vae_output - min_value) * (255 / (max_value - min_value))).clip(0, 255)\n",
    "    image = image.to(device=\"cpu\", dtype=th.uint8)\n",
    "    image = Image.fromarray(image.numpy())\n",
    "    return image\n",
    "\n",
    "num_heads = 8\n",
    "D = 1536\n",
    "J = 5\n",
    "head_dim = D // num_heads  # 192\n",
    "print(\"head_dim:\", head_dim)\n",
    "\n",
    "x_flat = rearrange(use_feats, 'n b f d -> b (n f) d').cuda().float()   # [1, 4096, 1536]\n",
    "print(\"x_flat shape:\", x_flat.shape)\n",
    "\n",
    "self_attn = SelfAttention(dim=D, num_heads=num_heads).to(x_flat.device)\n",
    "# decoder = Decoder3d(dim=96, z_dim=16, dim_mult=[1, 2, 4, 4], num_res_blocks=2,\n",
    "#                             attn_scales=[], temporal_upsample=[False, True, True])\n",
    "decoder = Decoder3d().cuda()\n",
    "\n",
    "# 1D RoPE over sequence length\n",
    "freqs = precompute_freqs_cis(head_dim, end=x_flat.shape[1], theta=10000.0).to(x_flat.device)\n",
    "print(\"SelfAttention:\", self_attn)\n",
    "print(\"freqs.shape:\", freqs.shape)\n",
    "print(\"x_flat.shape:\", x_flat.shape)\n",
    "\n",
    "freqs = freqs[None, :, None, :]\n",
    "out_flat = self_attn(x_flat, freqs=freqs)       # [1, 4096, 1536]\n",
    "print(\"out_flat shape:\", out_flat.shape)\n",
    "out = rearrange(out_flat, 'b (n f) d -> n b f d', n=len(use_time) * len(use_block), f=F)\n",
    "print(\"out shape:\", out.shape)\n",
    "\n",
    "grid_size = [16, 4, 4]\n",
    "patch_size = [1, 2, 2]\n",
    "\n",
    "head = th.nn.Linear(D, 64).to(out.device)\n",
    "out = head(out)\n",
    "print(\"out shape after head:\", out.shape)\n",
    "\n",
    "joint_head = th.nn.Linear(F, F * J).to(out.device)\n",
    "out = rearrange(out, 'n b f d -> n b d f')\n",
    "print(out.shape)\n",
    "out = joint_head(out)\n",
    "print(\"out shape after joint_head:\", out.shape)\n",
    "out_joint_combined = rearrange(out, 'n b d (j f) -> n b j f d', j=J)\n",
    "print(\"out_joint_combined shape:\", out_joint_combined.shape)\n",
    "\n",
    "\n",
    "#TODO: Should be replace with some layers, Pooling, etc. to combined the multiple time/block features\n",
    "out = out_joint_combined.squeeze(0)  # [B, F, C]\n",
    "print(\"out shape after squeeze:\", out.shape)\n",
    "out_unpatch = rearrange(\n",
    "            out, 'b j (f h w) (x y z c) -> b j c (f x) (h y) (w z)',\n",
    "            f=grid_size[0], h=grid_size[1], w=grid_size[2], \n",
    "            x=patch_size[0], y=patch_size[1], z=patch_size[2], j = J\n",
    "        )\n",
    "print(\"out_unpatch shape:\", out_unpatch.shape)\n",
    "\n",
    "# tiled = False\n",
    "# tile_size = None\n",
    "# tile_stride = None\n",
    "tiled = True,\n",
    "tile_size = (30, 52)\n",
    "tile_stride = (15, 26)\n",
    "video_vae = WanVideoVAE().cpu()\n",
    "out_unpatch = out_unpatch.cpu()\n",
    "\n",
    "out_joint = []\n",
    "print(\"out to decoding: \", out_unpatch.shape)\n",
    "print(\"out to decoding: \", out_unpatch[0:1, 0, ...].shape)\n",
    "assert False\n",
    "import tqdm\n",
    "for i in tqdm.tqdm(range(J), desc=\"Decoding joints\"):\n",
    "    video = video_vae.decode(out_unpatch[0:1, i, ...], device=out_unpatch.device, tiled=tiled, tile_size=tile_size, tile_stride=tile_stride)\n",
    "    out_joint.append(video)\n",
    "    print(\"video shape:\", video.shape)\n",
    "# video = vae_output_to_video(video)\n",
    "# print(\"video length:\", len(video))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39695516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(video.shape)\n",
    "plt.imshow(video[0][:, 0, ...].permute(1, 2, 0).cpu().detach().numpy())\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64704e8",
   "metadata": {},
   "source": [
    "Simple arch\n",
    "- Upsample + prediction head as 2N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce16ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[384, 384, 384, 192, 96]\n",
      "Upsample block 0: in_dim=384, out_dim=384, cur_dim=384\n",
      "Upsample block 1: in_dim=384, out_dim=384, cur_dim=192\n",
      "Upsample block 2: in_dim=384, out_dim=192, cur_dim=96\n",
      "Upsample block 3: in_dim=192, out_dim=96, cur_dim=48\n",
      "inp shape: torch.Size([1, 16, 16, 8, 8])\n",
      "conv1(inp).shape: torch.Size([1, 384, 16, 8, 8])\n",
      "16\n",
      "16\n",
      "16\n",
      "upsamples output shape: torch.Size([1, 48, 16, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# # upsample block\n",
    "# if i != len(dim_mult) - 1:\n",
    "#     mode = 'upsample3d' if temperal_upsample[i] else 'upsample2d'\n",
    "#     upsamples.append(Resample(out_dim, mode=mode))\n",
    "#     scale *= 2.0\n",
    "temperal_upsample = [True, True, False]\n",
    "from diffsynth.models.wan_video_vae import Decoder3d, CausalConv3d, VideoVAE_, WanVideoVAE, Resample\n",
    "dim_mult = [1, 2, 4, 4]\n",
    "dim = 96\n",
    "z_dim = 16\n",
    "tile_size\n",
    "dims = [dim * u for u in [dim_mult[-1]] + dim_mult[::-1]]\n",
    "print(dims)\n",
    "cur_dim = dims[0]  # whatever your starting channels are\n",
    "\n",
    "upsamples = []\n",
    "for i, (in_dim, out_dim) in enumerate(zip(dims[:-1], dims[1:])):\n",
    "    print(f\"Upsample block {i}: in_dim={in_dim}, out_dim={out_dim}, cur_dim={cur_dim}\")\n",
    "    if i != len(dim_mult) - 1:\n",
    "        mode = 'upsample3d' if temperal_upsample[i] else 'upsample2d'\n",
    "        upsamples.append(Resample(cur_dim, mode=mode))\n",
    "        cur_dim = cur_dim // 2   # <-- IMPORTANT: match Conv2d(cur_dim -> cur_dim//2)\n",
    "\n",
    "upsamples = th.nn.Sequential(*upsamples).cuda()\n",
    "conv1 = CausalConv3d(z_dim, dims[0], 3, padding=1).cuda()\n",
    "inp = th.randn(1, 16, 16, 8, 8).float().cuda()\n",
    "print(\"inp shape:\", inp.shape)\n",
    "print(\"conv1(inp).shape:\", conv1(inp).shape)\n",
    "# print(upsamples)\n",
    "# upsamples[1](upsamples[0](conv1(inp)))\n",
    "print(\"upsamples output shape:\", upsamples( conv1(inp) ).shape)\n",
    "\n",
    "#TODO: integrate with tiled decoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40e04cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[384, 384, 384, 192, 96]\n",
      "upsample3d\n",
      "upsample3d\n",
      "upsample2d\n",
      "Using tiled decoding torch.Size([1, 16, 16, 8, 8])\n",
      "T, H, W: 16 8 8\n",
      "weight shape: torch.Size([1, 1, 61, 64, 64])\n",
      "values shape: torch.Size([1, 3, 61, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VAE decoding:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding tile: h: 0-30, w: 0-52\n",
      "hidden_states.shape: torch.Size([1, 16, 16, 8, 8])\n",
      "Cleared cache: 4 [0] 4\n",
      "In VideoVAE_.decode: z's shape =   torch.Size([1, 16, 16, 8, 8])\n",
      "fw pass (conv1):  torch.Size([1, 384, 1, 8, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Sequential.forward() got an unexpected keyword argument 'feat_cache'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 188\u001b[39m\n\u001b[32m    183\u001b[39m inp = th.randn(\u001b[32m1\u001b[39m, \u001b[32m16\u001b[39m, \u001b[32m16\u001b[39m, \u001b[32m8\u001b[39m, \u001b[32m8\u001b[39m).float().cuda()  \u001b[38;5;66;03m# B, C, T, H, W\u001b[39;00m\n\u001b[32m    184\u001b[39m \u001b[38;5;66;03m# print(decoder.upsamples[0].mode)\u001b[39;00m\n\u001b[32m    185\u001b[39m \u001b[38;5;66;03m# decoder.upsamples(conv1(inp))\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;66;03m# print(decoder(inp).shape)\u001b[39;00m\n\u001b[32m    187\u001b[39m \u001b[38;5;66;03m# out = decoder.tiled_decode(inp, device=inp.device, tile_size=tile_size, tile_stride=tile_stride)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m188\u001b[39m out = \u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43minp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtiled\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtiled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtile_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtile_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtile_stride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtile_stride\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 74\u001b[39m, in \u001b[36mdecoder_simple.decode_fn\u001b[39m\u001b[34m(self, hidden_states, device, tiled, tile_size, tile_stride)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tiled:\n\u001b[32m     73\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mUsing tiled decoding\u001b[39m\u001b[33m\"\u001b[39m, hidden_state.shape)\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     video = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtiled_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtile_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtile_stride\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mvideo shape after tiled decode:\u001b[39m\u001b[33m\"\u001b[39m, video.shape)\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 137\u001b[39m, in \u001b[36mdecoder_simple.tiled_decode\u001b[39m\u001b[34m(self, hidden_states, device, tile_size, tile_stride)\u001b[39m\n\u001b[32m    135\u001b[39m hidden_states_batch = hidden_states[:, :, :, h:h_, w:w_].to(computation_device)\n\u001b[32m    136\u001b[39m \u001b[38;5;66;03m# hidden_states_batch = self.model.decode(hidden_states_batch, self.scale).to(data_device)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m hidden_states_batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtile_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtile_stride\u001b[49m\u001b[43m)\u001b[49m.to(data_device)\n\u001b[32m    138\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mdecoded hidden_states_batch.shape:\u001b[39m\u001b[33m\"\u001b[39m, hidden_states_batch.shape)\n\u001b[32m    140\u001b[39m mask = \u001b[38;5;28mself\u001b[39m.build_mask(\n\u001b[32m    141\u001b[39m     hidden_states_batch,\n\u001b[32m    142\u001b[39m     is_bound=(h==\u001b[32m0\u001b[39m, h_>=H, w==\u001b[32m0\u001b[39m, w_>=W),\n\u001b[32m    143\u001b[39m     border_width=((size_h - stride_h) * \u001b[38;5;28mself\u001b[39m.upsampling_factor, (size_w - stride_w) * \u001b[38;5;28mself\u001b[39m.upsampling_factor)\n\u001b[32m    144\u001b[39m ).to(dtype=hidden_states.dtype, device=data_device)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 92\u001b[39m, in \u001b[36mdecoder_simple.decode\u001b[39m\u001b[34m(self, z, tile_size, tile_stride)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28mself\u001b[39m._conv_idx = [\u001b[32m0\u001b[39m]\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m                       \u001b[49m\u001b[43mfeat_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_feat_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m                       \u001b[49m\u001b[43mfeat_idx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     96\u001b[39m     out_ = \u001b[38;5;28mself\u001b[39m.forward_pass(x[:, :, i:i + \u001b[32m1\u001b[39m, :, :],\n\u001b[32m     97\u001b[39m                         feat_cache=\u001b[38;5;28mself\u001b[39m._feat_map,\n\u001b[32m     98\u001b[39m                         feat_idx=\u001b[38;5;28mself\u001b[39m._conv_idx)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 64\u001b[39m, in \u001b[36mdecoder_simple.forward_pass\u001b[39m\u001b[34m(self, x, feat_cache, feat_idx)\u001b[39m\n\u001b[32m     62\u001b[39m x = \u001b[38;5;28mself\u001b[39m.conv1(x)\n\u001b[32m     63\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mfw pass (conv1): \u001b[39m\u001b[33m\"\u001b[39m, x.shape)\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresample_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeat_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeat_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeat_idx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeat_idx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ist-nas/users/puntawatp/conda_envs/skelag/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ist-nas/users/puntawatp/conda_envs/skelag/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[31mTypeError\u001b[39m: Sequential.forward() got an unexpected keyword argument 'feat_cache'"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "def count_conv3d(model):\n",
    "    count = 0\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, CausalConv3d):\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "class decoder_simple(th.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        dim_mult = [1, 2, 4, 4]\n",
    "        dim = 96\n",
    "        z_dim = 16\n",
    "        tile_size\n",
    "        dims = [dim * u for u in [dim_mult[-1]] + dim_mult[::-1]]\n",
    "        self.upsampling_factor = 8\n",
    "        print(dims)\n",
    "        cur_dim = dims[0]  # whatever your starting channels are\n",
    "        temperal_upsample = [False, True, True][::-1]\n",
    "\n",
    "        upsamples = []\n",
    "        for i, (in_dim, out_dim) in enumerate(zip(dims[:-1], dims[1:])):\n",
    "            if i != len(dim_mult) - 1:\n",
    "                mode = 'upsample3d' if temperal_upsample[i] else 'upsample2d'\n",
    "                print(mode)\n",
    "                upsamples.append(Resample(cur_dim, mode=mode))\n",
    "                cur_dim = cur_dim // 2   # <-- IMPORTANT: match Conv2d(cur_dim -> cur_dim//2)\n",
    "\n",
    "        self.resample_module = th.nn.Sequential(*upsamples).cuda()\n",
    "        self.conv1 = CausalConv3d(z_dim, dims[0], 3, padding=1).cuda()\n",
    "        self.conv2 = CausalConv3d(z_dim, z_dim, 1)\n",
    "\n",
    "    def clear_cache(self):\n",
    "        self._conv_num = count_conv3d(self.resample_module) + count_conv3d(self.conv1) + count_conv3d(self.conv2)\n",
    "        self._conv_idx = [0]\n",
    "        self._feat_map = [None] * self._conv_num\n",
    "        print(\"Cleared cache:\", self._conv_num, self._conv_idx, len(self._feat_map))\n",
    "    \n",
    "    def build_1d_mask(self, length, left_bound, right_bound, border_width):\n",
    "        x = th.ones((length,))\n",
    "        if not left_bound:\n",
    "            x[:border_width] = (th.arange(border_width) + 1) / border_width\n",
    "        if not right_bound:\n",
    "            x[-border_width:] = th.flip((th.arange(border_width) + 1) / border_width, dims=(0,))\n",
    "        return x\n",
    "\n",
    "\n",
    "    def build_mask(self, data, is_bound, border_width):\n",
    "        _, _, _, H, W = data.shape\n",
    "        h = self.build_1d_mask(H, is_bound[0], is_bound[1], border_width[0])\n",
    "        w = self.build_1d_mask(W, is_bound[2], is_bound[3], border_width[1])\n",
    "\n",
    "        h = repeat(h, \"H -> H W\", H=H, W=W)\n",
    "        w = repeat(w, \"W -> H W\", H=H, W=W)\n",
    "\n",
    "        mask = th.stack([h, w]).min(dim=0).values\n",
    "        mask = rearrange(mask, \"H W -> 1 1 1 H W\")\n",
    "        return mask\n",
    "        \n",
    "    def forward_pass(self, x, feat_cache=None, feat_idx=[0]):\n",
    "        x = self.conv1(x)\n",
    "        print(\"fw pass (conv1): \", x.shape)\n",
    "        return self.resample_module(x, feat_cache=feat_cache, feat_idx=feat_idx)\n",
    "    \n",
    "    def decode_fn(self, hidden_states, device, tiled=False, tile_size=(34, 34), tile_stride=(18, 16)):\n",
    "        #NOTE: From WanVideoVAE.decode\n",
    "        hidden_states = [hidden_state.to(\"cpu\") for hidden_state in hidden_states]  # batched tensor -> list of tensor\n",
    "        videos = []\n",
    "        for hidden_state in hidden_states:\n",
    "            hidden_state = hidden_state.unsqueeze(0)    # add batch dim\n",
    "            if tiled:\n",
    "                print(\"Using tiled decoding\", hidden_state.shape)\n",
    "                video = self.tiled_decode(hidden_state, device, tile_size, tile_stride)\n",
    "                print(\"video shape after tiled decode:\", video.shape)\n",
    "            else:\n",
    "                video = self.single_decode(hidden_state, device)\n",
    "            video = video.squeeze(0)\n",
    "            videos.append(video)\n",
    "        videos = th.stack(videos)\n",
    "        return videos\n",
    "    \n",
    "    def decode(self, z, tile_size, tile_stride):\n",
    "        self.clear_cache()\n",
    "        #NOTE: From VideoVAE_.decode\n",
    "        print(\"In VideoVAE_.decode: z's shape =  \", z.shape)\n",
    "        iter_ = z.shape[2]\n",
    "        x = self.conv2(z)\n",
    "        for i in range(iter_):\n",
    "            self._conv_idx = [0]\n",
    "            if i == 0:\n",
    "                out = self.forward_pass(x[:, :, i:i + 1, :, :],\n",
    "                                   feat_cache=self._feat_map,\n",
    "                                   feat_idx=self._conv_idx)\n",
    "            else:\n",
    "                out_ = self.forward_pass(x[:, :, i:i + 1, :, :],\n",
    "                                    feat_cache=self._feat_map,\n",
    "                                    feat_idx=self._conv_idx)\n",
    "                out = th.cat([out, out_], 2) # may add tensor offload\n",
    "        # for i in range(iter_):\n",
    "            # print(f\"{i} - {x[:, :, i:i+1, :, :].shape}\")\n",
    "        #     out = self.model(x[:, :, i:i+1, :, :])\n",
    "        #     all_out.append(out)\n",
    "        # all_out = th.cat(all_out, dim=2)\n",
    "        print(\"In VideoVAE_.decode: all_out's shape = \", out.shape)\n",
    "        return out\n",
    "    \n",
    "    def tiled_decode(self, hidden_states, device, tile_size, tile_stride):\n",
    "        _, _, T, H, W = hidden_states.shape\n",
    "        print(\"T, H, W:\", T, H, W)\n",
    "        size_h, size_w = tile_size\n",
    "        stride_h, stride_w = tile_stride\n",
    "\n",
    "        # Split tasks\n",
    "        tasks = []\n",
    "        for h in range(0, H, stride_h):\n",
    "            if (h-stride_h >= 0 and h-stride_h+size_h >= H): continue\n",
    "            for w in range(0, W, stride_w):\n",
    "                if (w-stride_w >= 0 and w-stride_w+size_w >= W): continue\n",
    "                h_, w_ = h + size_h, w + size_w\n",
    "                tasks.append((h, h_, w, w_))\n",
    "\n",
    "        data_device = \"cpu\"\n",
    "        computation_device = device\n",
    "\n",
    "        out_T = T * 4 - 3\n",
    "        weight = th.zeros((1, 1, out_T, H * self.upsampling_factor, W * self.upsampling_factor), dtype=hidden_states.dtype, device=data_device)\n",
    "        values = th.zeros((1, 3, out_T, H * self.upsampling_factor, W * self.upsampling_factor), dtype=hidden_states.dtype, device=data_device)\n",
    "        print(\"weight shape:\", weight.shape)\n",
    "        print(\"values shape:\", values.shape)\n",
    "\n",
    "        for h, h_, w, w_ in tqdm.tqdm(tasks, desc=\"VAE decoding\"):\n",
    "            print(f\"Decoding tile: h: {h}-{h_}, w: {w}-{w_}\")\n",
    "            print(\"hidden_states.shape:\", hidden_states.shape)\n",
    "            hidden_states_batch = hidden_states[:, :, :, h:h_, w:w_].to(computation_device)\n",
    "            # hidden_states_batch = self.model.decode(hidden_states_batch, self.scale).to(data_device)\n",
    "            hidden_states_batch = self.decode(hidden_states_batch, tile_size, tile_stride).to(data_device)\n",
    "            print(\"decoded hidden_states_batch.shape:\", hidden_states_batch.shape)\n",
    "\n",
    "            mask = self.build_mask(\n",
    "                hidden_states_batch,\n",
    "                is_bound=(h==0, h_>=H, w==0, w_>=W),\n",
    "                border_width=((size_h - stride_h) * self.upsampling_factor, (size_w - stride_w) * self.upsampling_factor)\n",
    "            ).to(dtype=hidden_states.dtype, device=data_device)\n",
    "            \n",
    "            print(\"mask.shape:\", mask.shape)\n",
    "            print(\"values.shape:\", values.shape)\n",
    "            print(\"hidden_states_batch.shape:\", hidden_states_batch.shape)\n",
    "            print(\"weight.shape:\", weight.shape)\n",
    "            \n",
    "\n",
    "            target_h = h * self.upsampling_factor\n",
    "            target_w = w * self.upsampling_factor\n",
    "            values[\n",
    "                :,\n",
    "                :,\n",
    "                :,\n",
    "                target_h:target_h + hidden_states_batch.shape[3],\n",
    "                target_w:target_w + hidden_states_batch.shape[4],\n",
    "            ] += hidden_states_batch * mask\n",
    "            weight[\n",
    "                :,\n",
    "                :,\n",
    "                :,\n",
    "                target_h: target_h + hidden_states_batch.shape[3],\n",
    "                target_w: target_w + hidden_states_batch.shape[4],\n",
    "            ] += mask\n",
    "        values = values / weight\n",
    "        values = values.clamp_(-1, 1)\n",
    "        print(values.shape)\n",
    "        return values\n",
    "    \n",
    "    def single_decode(self, hidden_state, device):\n",
    "        hidden_state = hidden_state.to(device)\n",
    "        video = self.model.decode(hidden_state, self.scale)\n",
    "        return video.clamp_(-1, 1)\n",
    "    \n",
    "\n",
    "tiled = True,\n",
    "tile_size = (30, 52)\n",
    "tile_stride = (15, 26)\n",
    "decoder = decoder_simple().cuda()\n",
    "inp = th.randn(1, 16, 16, 8, 8).float().cuda()  # B, C, T, H, W\n",
    "# print(decoder.upsamples[0].mode)\n",
    "# decoder.upsamples(conv1(inp))\n",
    "# print(decoder(inp).shape)\n",
    "# out = decoder.tiled_decode(inp, device=inp.device, tile_size=tile_size, tile_stride=tile_stride)\n",
    "out = decoder.decode_fn(inp, device=inp.device, tiled=tiled, tile_size=tile_size, tile_stride=tile_stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d712c519",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = th.randn(10, 16, 16, 8, 8).float().cuda()  # B, C, T, H, W\n",
    "print(\"hidden_states shape:\", hidden_states.shape)\n",
    "hidden_states = [hidden_state.to(\"cpu\") for hidden_state in hidden_states]\n",
    "print(len(hidden_states))\n",
    "print(\"hidden_states[0] shape:\", hidden_states[0].shape)\n",
    "_, _, T, H, W = hidden_states[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "eaea736e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[384, 384, 384, 192, 96]\n",
      "Upsample block 0: in_dim=384, out_dim=384, cur_dim=384\n",
      "Upsample block 1: in_dim=384, out_dim=384, cur_dim=192\n",
      "Upsample block 2: in_dim=384, out_dim=192, cur_dim=96\n",
      "Upsample block 3: in_dim=192, out_dim=96, cur_dim=48\n",
      "CausalConv3d(16, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "inp shape: torch.Size([1, 16, 16, 8, 8])\n",
      "conv1(inp).shape: torch.Size([1, 384, 16, 8, 8])\n",
      "inp:  torch.Size([1, 16, 16, 8, 8])\n",
      "inp after time_conv shape: torch.Size([1, 32, 16, 8, 8])\n",
      "after reshape:  torch.Size([1, 2, 16, 16, 8, 8])\n",
      "after stack:  torch.Size([1, 16, 16, 2, 8, 8])\n",
      "after final reshape:  torch.Size([1, 16, 32, 8, 8])\n",
      "before upsample:  torch.Size([32, 16, 8, 8])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 5, got 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[70]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     47\u001b[39m x = rearrange(x, \u001b[33m'\u001b[39m\u001b[33mb c t h w -> (b t) c h w\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     48\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mbefore upsample: \u001b[39m\u001b[33m\"\u001b[39m, x.shape)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m x = \u001b[43mupsamples\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mafter upsample: \u001b[39m\u001b[33m\"\u001b[39m, x.shape)\n\u001b[32m     51\u001b[39m x = rearrange(x, \u001b[33m'\u001b[39m\u001b[33m(b t) c h w -> b c t h w\u001b[39m\u001b[33m'\u001b[39m, t=t)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ist-nas/users/puntawatp/conda_envs/skelag/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ist-nas/users/puntawatp/conda_envs/skelag/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/SkelAg/DiffSynth-Studio/diffsynth/models/wan_video_vae.py:121\u001b[39m, in \u001b[36mResample.forward\u001b[39m\u001b[34m(self, x, feat_cache, feat_idx)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, feat_cache=\u001b[38;5;28;01mNone\u001b[39;00m, feat_idx=[\u001b[32m0\u001b[39m]):\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m     b, c, t, h, w = x.size()\n\u001b[32m    122\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode == \u001b[33m'\u001b[39m\u001b[33mupsample3d\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    123\u001b[39m         \u001b[38;5;66;03m#TODO: feat_cache is neede for upsample3d (timeconv), so what's cache logic here?\u001b[39;00m\n\u001b[32m    124\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m feat_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mValueError\u001b[39m: not enough values to unpack (expected 5, got 4)"
     ]
    }
   ],
   "source": [
    "# # upsample block\n",
    "# if i != len(dim_mult) - 1:\n",
    "#     mode = 'upsample3d' if temperal_upsample[i] else 'upsample2d'\n",
    "#     upsamples.append(Resample(out_dim, mode=mode))\n",
    "#     scale *= 2.0\n",
    "temperal_upsample = [True, True, False]\n",
    "from diffsynth.models.wan_video_vae import Decoder3d, CausalConv3d, VideoVAE_, WanVideoVAE, Resample\n",
    "dim_mult = [1, 2, 4, 4]\n",
    "dim = 96\n",
    "z_dim = 16\n",
    "tile_size\n",
    "dims = [dim * u for u in [dim_mult[-1]] + dim_mult[::-1]]\n",
    "print(dims)\n",
    "cur_dim = dims[0]  # whatever your starting channels are\n",
    "\n",
    "upsamples = []\n",
    "for i, (in_dim, out_dim) in enumerate(zip(dims[:-1], dims[1:])):\n",
    "    print(f\"Upsample block {i}: in_dim={in_dim}, out_dim={out_dim}, cur_dim={cur_dim}\")\n",
    "    if i != len(dim_mult) - 1:\n",
    "        mode = 'upsample3d' if temperal_upsample[i] else 'upsample2d'\n",
    "        upsamples.append(Resample(cur_dim, mode=mode))\n",
    "        cur_dim = cur_dim // 2   # <-- IMPORTANT: match Conv2d(cur_dim -> cur_dim//2)\n",
    "# print(upsamples)\n",
    "time_conv = CausalConv3d(16,\n",
    "                            16 * 2, (3, 1, 1),\n",
    "                            padding=(1, 0, 0)).cuda()\n",
    "upsamples = th.nn.Sequential(*upsamples).cuda()\n",
    "conv1 = CausalConv3d(z_dim, dims[0], 3, padding=1).cuda()\n",
    "print(conv1)\n",
    "inp = th.randn(1, 16, 16, 8, 8).float().cuda()\n",
    "print(\"inp shape:\", inp.shape)\n",
    "print(\"conv1(inp).shape:\", conv1(inp).shape)\n",
    "b, c, t, h, w = inp.shape\n",
    "print(\"inp: \", inp.shape)\n",
    "x = time_conv(inp)\n",
    "# b, c, t, h, w = x.shape\n",
    "print(\"inp after time_conv shape:\", x.shape)\n",
    "x = x.reshape(b, 2, c, t, h, w)\n",
    "print(\"after reshape: \", x.shape)\n",
    "x = th.stack((x[:, 0, :, :, :, :], x[:, 1, :, :, :, :]),\n",
    "                3)\n",
    "print(\"after stack: \", x.shape)\n",
    "x = x.reshape(b, c, t * 2, h, w)\n",
    "print(\"after final reshape: \", x.shape)\n",
    "#TODO: integrate with tiled decoding\n",
    "t = x.shape[2]\n",
    "x = rearrange(x, 'b c t h w -> (b t) c h w')\n",
    "print(\"before upsample: \", x.shape)\n",
    "x = upsamples[0](x)\n",
    "print(\"after upsample: \", x.shape)\n",
    "x = rearrange(x, '(b t) c h w -> b c t h w', t=t)\n",
    "print(\"after upsample reshape: \", x.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2fb4f5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[384, 384, 384, 192, 96]\n",
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (0): Upsample(scale_factor=(2.0, 2.0), mode='nearest-exact')\n",
      "    (1): Conv2d(384, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (1): CausalConv3d(192, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "  (2): Sequential(\n",
      "    (0): Upsample(scale_factor=(2.0, 2.0), mode='nearest-exact')\n",
      "    (1): Conv2d(384, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (3): CausalConv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "  (4): Sequential(\n",
      "    (0): Upsample(scale_factor=(2.0, 2.0), mode='nearest-exact')\n",
      "    (1): Conv2d(192, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (5): CausalConv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      ")\n",
      "inp to conv1:  torch.Size([1, 16, 61, 8, 8])\n",
      "output from conv1:  torch.Size([1, 384, 61, 8, 8])\n",
      "inp to rearrange:  torch.Size([1, 384, 61, 8, 8])\n",
      "inp to resample:  torch.Size([61, 384, 8, 8])\n",
      "Sequential(\n",
      "  (0): Upsample(scale_factor=(2.0, 2.0), mode='nearest-exact')\n",
      "  (1): Conv2d(384, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "after resample:  torch.Size([61, 192, 16, 16])\n",
      "torch.Size([1, 192, 61, 16, 16])\n",
      "inp to rearrange:  torch.Size([1, 192, 61, 16, 16])\n",
      "inp to resample:  torch.Size([1, 192, 61, 16, 16])\n",
      "CausalConv3d(192, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "after resample:  torch.Size([1, 384, 61, 16, 16])\n",
      "torch.Size([1, 384, 61, 16, 16])\n",
      "inp to rearrange:  torch.Size([1, 384, 61, 16, 16])\n",
      "inp to resample:  torch.Size([61, 384, 16, 16])\n",
      "Sequential(\n",
      "  (0): Upsample(scale_factor=(2.0, 2.0), mode='nearest-exact')\n",
      "  (1): Conv2d(384, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "after resample:  torch.Size([61, 192, 32, 32])\n",
      "torch.Size([1, 192, 61, 32, 32])\n",
      "inp to rearrange:  torch.Size([1, 192, 61, 32, 32])\n",
      "inp to resample:  torch.Size([1, 192, 61, 32, 32])\n",
      "CausalConv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "after resample:  torch.Size([1, 192, 61, 32, 32])\n",
      "torch.Size([1, 192, 61, 32, 32])\n",
      "inp to rearrange:  torch.Size([1, 192, 61, 32, 32])\n",
      "inp to resample:  torch.Size([61, 192, 32, 32])\n",
      "Sequential(\n",
      "  (0): Upsample(scale_factor=(2.0, 2.0), mode='nearest-exact')\n",
      "  (1): Conv2d(192, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "after resample:  torch.Size([61, 96, 64, 64])\n",
      "torch.Size([1, 96, 61, 64, 64])\n",
      "inp to rearrange:  torch.Size([1, 96, 61, 64, 64])\n",
      "inp to resample:  torch.Size([1, 96, 61, 64, 64])\n",
      "CausalConv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "after resample:  torch.Size([1, 96, 61, 64, 64])\n",
      "torch.Size([1, 96, 61, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from diffsynth.models.wan_video_vae import Upsample\n",
    "x = th.randn(1, 16, 16, 8, 8).float().cuda()\n",
    "b, c, t, h, w = x.shape\n",
    "x = F.interpolate(x, size=(t*4-3, h, w), mode=\"trilinear\", align_corners=True)\n",
    "resample = th.nn.Sequential(\n",
    "        Upsample(scale_factor=(2., 2.), mode='nearest-exact'),\n",
    "        th.nn.Conv2d(dims[0], dims[0] // 2, 3, padding=1)).cuda()\n",
    "conv1 = CausalConv3d(z_dim, dims[0], 3, padding=1).cuda()\n",
    "upsample_dim = 96\n",
    "dim_mult = [1, 2, 4, 4]\n",
    "upsample_dims = [upsample_dim * u for u in [dim_mult[-1]] + dim_mult[::-1]]\n",
    "print(upsample_dims)\n",
    "# Upsample layers\n",
    "upsamples = []\n",
    "for i, (in_dim, out_dim) in enumerate(zip(upsample_dims[:-1], upsample_dims[1:])):\n",
    "    # print(f\"Upsample block {i}: in_dim={in_dim}, out_dim={out_dim}, cur_dim={cur_dim}\")\n",
    "        if i == 1 or i == 2 or i == 3:\n",
    "            in_dim = in_dim // 2\n",
    "            upsamples.append(CausalConv3d(in_dim, out_dim, 3, padding=1))\n",
    "        if i != len(dim_mult) - 1:\n",
    "                upsamples.append(th.nn.Sequential(\n",
    "                        Upsample(scale_factor=(2., 2.), mode='nearest-exact'),\n",
    "                        th.nn.Conv2d(out_dim, out_dim // 2, 3, padding=1)))\n",
    "\n",
    "    # cur_dim = cur_dim // 2   # <-- IMPORTANT: match Conv2d(cur_dim -> cur_dim//2)\n",
    "upsamples = th.nn.Sequential(*upsamples).cuda()\n",
    "print(upsamples)\n",
    "# Joint heatmap final conv\n",
    "joint_map_conv = th.nn.Conv2d(upsample_dims[-1],  2 * J, 3, padding=1).cuda()\n",
    "print(\"inp to conv1: \", x.shape)\n",
    "x = conv1(x)\n",
    "print(\"output from conv1: \", x.shape)\n",
    "t = x.shape[2]\n",
    "for layer in upsamples:\n",
    "     print(\"inp to rearrange: \", x.shape)\n",
    "     if not isinstance(layer, CausalConv3d):\n",
    "        x = rearrange(x, 'b c t h w -> (b t) c h w')\n",
    "     print(\"inp to resample: \", x.shape)\n",
    "     print(layer)\n",
    "     x = layer(x)\n",
    "     print(\"after resample: \", x.shape)\n",
    "     if not isinstance(layer, CausalConv3d):\n",
    "        x = rearrange(x, '(b t) c h w -> b c t h w', t=t)\n",
    "     print(x.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skelag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
